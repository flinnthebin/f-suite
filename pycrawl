#!/usr/bin/python
"""
flag_spider.py

This script uses a client PEM certificate to crawl the target domain.
For every requested page it:
  • Prints the HTTP status code.
  • Waits a given delay between requests (starting at 0.5s) and will increase
    the delay if a 429 (Too Many Requests) response is received.
  • Searches the page content for a flag using a regex.
     If a positive match is found, the flag is printed and the script exits.
  • Otherwise, it extracts all links from the page (limiting itself to URLs on the same domain)
    and adds new links to the crawl queue.
  • In addition, it uses a supplied wordlist (in this example a small directory list)
    to generate additional URLs to check.

Usage:
  ./flag_spider.py

Make sure that:
  • Your PEM file is accessible (it must include the private key if required).
  • The wordlist file exists at the indicated path.
"""

import re
import sys
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# ----- Configuration -----
cert = "/home/archer/uni/COMP6843/z3291100.pem"
flag_pattern = re.compile(r"COMP6443\{.*?\}")
wordlist = "/home/archer/uni/COMP6843/visited_urls.txt"
target = "https://spiderman.quoccacorp.com"
initial_delay = 0.5

# ----- Set up a requests session with our client certificate -----
session = requests.Session()
session.cert = cert

# ----- Global Variables for crawling -----
visited = set()
to_visit = [target]
current_delay = initial_delay


def process_url(url):
    """
    Request the given URL using the session. Print the HTTP status code.
    If the response content matches the flag regex, print the flag and exit.

    Returns the response text (or an empty string on error) and the status code.
    Adjusts the current_delay based on server feedback.
    """
    global current_delay
    print(f"\nRequesting {url} ...")
    try:
        resp = session.get(url, timeout=10)
        print(f"HTTP {resp.status_code} for {url}")
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        current_delay = min(current_delay * 2, 10)
        return "", None

    content = resp.text
    match = flag_pattern.search(content)
    if match:
        print("\n*** Flag found! ***")
        print(match.group())
        sys.exit(0)

    if resp.status_code == 429:
        current_delay = min(current_delay * 2, 10)
        print(f"Rate limited! Increasing delay to {
              current_delay:.1f} seconds.")
    else:
        current_delay = initial_delay
    return content, resp.status_code


def extract_links(html, base_url):
    """
    Extract all <a href=""> links from HTML, resolve them to absolute URLs,
    and return only those that appear to belong to the target domain.
    """
    links = set()
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup.find_all("a", href=True):
        href = tag["href"]
        absolute = urljoin(base_url, href)
        parsed_abs = urlparse(absolute)
        parsed_target = urlparse(target)
        if parsed_abs.scheme in ("http", "https") and parsed_abs.netloc.endswith(parsed_target.netloc):
            links.add(absolute)
    return links


def generate_wordlist_urls():
    """
    Read the wordlist file and generate a list of additional URLs by appending
    each word (as a directory) to the target domain.
    """
    urls = []
    try:
        with open(wordlist, "r") as f:
            for line in f:
                word = line.strip()
                if word:
                    new_url = urljoin(target + "/", word)
                    urls.append(new_url)
    except Exception as e:
        print(f"Error reading wordlist: {e}")
    return urls


to_visit.extend(generate_wordlist_urls())

# ----- Main Crawling Loop -----
while to_visit:
    current_url = to_visit.pop(0)
    if current_url in visited:
        continue
    visited.add(current_url)

    html, status = process_url(current_url)
    time.sleep(current_delay)

    if html:
        new_links = extract_links(html, current_url)
        for link in new_links:
            if link not in visited:
                to_visit.append(link)

print("\nCrawling complete. Flag not found.")
sys.exit(1)
