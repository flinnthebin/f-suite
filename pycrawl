#!/usr/bin/python
"""
pycrawl
"""

import re
import sys
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# ----- Optional command line argument for target -----
if len(sys.argv) != 2:
    print("Usage: {} URL".format(sys.argv[0]))
    sys.exit(1)

target = sys.argv[1]
wordlist = "visited_urls.txt"   # use the output from wget-spider
cert = "/home/archer/uni/COMP6843/z3291100.pem"
flag_pattern = re.compile(r"COMP6443\{.*?\}")

# ----- Set up a requests session with our client certificate -----
session = requests.Session()
session.cert = cert

# ----- Global Variables for crawling -----
visited = set()
to_visit = [target]


def process_url(url):
    """
    Request the given URL using the session. Print the HTTP status code.
    If the response content matches the flag regex, print the flag and exit.

    Returns the response text (or an empty string on error) and the status code.
    """
    print(f"\nRequesting {url} ...")
    try:
        resp = session.get(url, timeout=10)
        print(f"HTTP {resp.status_code} for {url}")
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return "", None

    content = resp.text
    match = flag_pattern.search(content)
    if match:
        print("\n*** Flag found! ***")
        print(match.group())
        sys.exit(0)

    return content, resp.status_code


def extract_links(html, base_url):
    """
    Extract all <a href=""> links from HTML, resolve them to absolute URLs,
    and return only those that appear to belong to the target domain.
    """
    links = set()
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup.find_all("a", href=True):
        href = tag["href"]
        absolute = urljoin(base_url, href)
        parsed_abs = urlparse(absolute)
        parsed_target = urlparse(target)
        if parsed_abs.scheme in ("http", "https") and parsed_abs.netloc.endswith(parsed_target.netloc):
            links.add(absolute)
    return links


def generate_wordlist_urls():
    """
    Read the wordlist file and generate a list of additional URLs by appending
    each word (as a directory) to the target domain.
    """
    urls = []
    try:
        with open(wordlist, "r") as f:
            for line in f:
                word = line.strip()
                if word:
                    new_url = urljoin(target + "/", word)
                    urls.append(new_url)
    except Exception as e:
        print(f"Error reading wordlist: {e}")
    return urls


to_visit.extend(generate_wordlist_urls())

# ----- Main Crawling Loop -----
while to_visit:
    current_url = to_visit.pop(0)
    if current_url in visited:
        continue
    visited.add(current_url)

    html, status = process_url(current_url)

    if html:
        new_links = extract_links(html, current_url)
        for link in new_links:
            if link not in visited:
                to_visit.append(link)

print("\nCrawling complete. Flag not found.")
sys.exit(1)
